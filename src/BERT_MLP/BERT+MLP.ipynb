{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT+MLP.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the notebook"
      ],
      "metadata": {
        "id": "I3yahGLWYq4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make sure you are connected to a GPU to make use of hardware acceleration\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "m5Dc77tzdQpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78967609-2169-43c6-efbd-6d5c4d62f968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jul 15 12:36:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "import torch, gc\n",
        "import os\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "o4hVGzzP0i_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "EX4hVMTQdRsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d058ab54-e44b-4745-99b1-58e4a86261fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install and import \n",
        "!pip install transformers\n",
        "from transformers import BertConfig\n",
        "from transformers import BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler"
      ],
      "metadata": {
        "id": "DkdYmIBvLHt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ddc0e8-d033-4a2d-af83-fa06e7ac254c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in data\n"
      ],
      "metadata": {
        "id": "XM6WYfgqaJRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpdChvMeFf_d",
        "outputId": "9e96b8d4-03de-4ca4-f9f7-a706e777b1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbqL5nkW7-52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "outputId": "05d95fee-db6f-4ef9-8da5-950b10a6b8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading csv files...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0  label                          chatName  \\\n",
              "0               0      0  9fbb1cac6583f9df43d968c02cfc9eff   \n",
              "1               1      0  afcdee26c7ba50454fe886e80eb16ded   \n",
              "2               2      1                      aggresk8er-0   \n",
              "3               3      1                      aggresk8er-1   \n",
              "4               4      1                      aggresk8er-4   \n",
              "...           ...    ...                               ...   \n",
              "13154       13154      0  6377d2c072798e7ef0fd22f0769fdbae   \n",
              "13155       13155      0  a2378441841247f277c54964860db8c9   \n",
              "13156       13156      0  497ec625519d6e28dd360be81d01dbbd   \n",
              "13157       13157      0  dfbec532af50f894e0961bb5ef935c4c   \n",
              "13158       13158      0  57c30df4a8ebe316e03bb7d823de964f   \n",
              "\n",
              "                                                 segment  length_rel  \\\n",
              "0      [CLS] Hey hiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii m/...    0.277439   \n",
              "1      [CLS] hi hi easter is gay how are u? yea agree...    0.572314   \n",
              "2      [CLS] hello how are you hi how are you goo goo...    0.026667   \n",
              "3      [CLS] Hello hi what ya doin? Nothing much just...    0.403315   \n",
              "4      [CLS] Hello hi How r u doing good what u been ...    0.016981   \n",
              "...                                                  ...         ...   \n",
              "13154  [CLS] 15 m hey im 15 f from hey im bobby usa c...    0.131579   \n",
              "13155  [CLS] hiii hw r u hiiii are you okay ya u too ...    0.109890   \n",
              "13156  [CLS] Why should interruptions be more problem...    0.811240   \n",
              "13157  [CLS] boo aaaaaaaaaah :o you scared me Yes I s...    0.060071   \n",
              "13158  [CLS] 21 m M or f Ok. F m age? Are looking for...    0.166667   \n",
              "\n",
              "       length_abs  num_questions_rel  num_questions_abs  size_of_words_rel  \\\n",
              "0           565.0           1.000000                3.0           0.280625   \n",
              "1           691.0           0.000000                4.0           0.369517   \n",
              "2          1184.0           0.857143               16.0           0.260687   \n",
              "3           289.0           0.500000                3.0           0.255186   \n",
              "4          1051.0           1.000000                9.0           0.248014   \n",
              "...           ...                ...                ...                ...   \n",
              "13154       994.0           0.857143                8.0           0.227453   \n",
              "13155       172.0           0.000000                0.0           0.123126   \n",
              "13156      3067.0           0.333333                5.0           0.808098   \n",
              "13157       549.0           0.750000                5.0           0.315303   \n",
              "13158       407.0           0.400000                8.0           0.007870   \n",
              "\n",
              "       size_of_words_abs  ...  bert_nervousness  bert_optimism  bert_pride  \\\n",
              "0             139.957143  ...          0.002213       0.002550    0.000968   \n",
              "1             148.843651  ...          0.002361       0.065858    0.004021   \n",
              "2             320.263490  ...          0.000863       0.003288    0.002020   \n",
              "3              69.497619  ...          0.000915       0.004819    0.000722   \n",
              "4             221.249451  ...          0.000613       0.012490    0.003142   \n",
              "...                  ...  ...               ...            ...         ...   \n",
              "13154         221.526190  ...          0.001099       0.007855    0.001591   \n",
              "13155          68.850000  ...          0.000959       0.002663    0.001041   \n",
              "13156         283.292332  ...          0.002674       0.010463    0.002572   \n",
              "13157         137.597619  ...          0.006449       0.006909    0.005956   \n",
              "13158         116.025000  ...          0.006828       0.010044    0.001704   \n",
              "\n",
              "       bert_realization  bert_relief  bert_remorse  bert_sadness  \\\n",
              "0              0.007122     0.000705      0.001172      0.004991   \n",
              "1              0.018808     0.002213      0.001050      0.003822   \n",
              "2              0.007620     0.001647      0.001496      0.005680   \n",
              "3              0.005034     0.000704      0.015193      0.013506   \n",
              "4              0.004357     0.002666      0.000648      0.001321   \n",
              "...                 ...          ...           ...           ...   \n",
              "13154          0.011055     0.000818      0.000697      0.002752   \n",
              "13155          0.008098     0.000736      0.000540      0.004075   \n",
              "13156          0.024410     0.001482      0.001058      0.005327   \n",
              "13157          0.014781     0.020089      0.002330      0.009367   \n",
              "13158          0.028492     0.001058      0.002017      0.009619   \n",
              "\n",
              "       bert_surprise  bert_neutral  split  \n",
              "0           0.008361      0.077860  train  \n",
              "1           0.002383      0.109519  train  \n",
              "2           0.004913      0.017312  train  \n",
              "3           0.002184      0.022046  train  \n",
              "4           0.002366      0.013954  train  \n",
              "...              ...           ...    ...  \n",
              "13154       0.002364      0.075391   test  \n",
              "13155       0.000781      0.773637   test  \n",
              "13156       0.004805      0.375396   test  \n",
              "13157       0.004675      0.026990   test  \n",
              "13158       0.006185      0.220681   test  \n",
              "\n",
              "[30574 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67f00449-3e62-4e6c-b2eb-a3619503ea9a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>chatName</th>\n",
              "      <th>segment</th>\n",
              "      <th>length_rel</th>\n",
              "      <th>length_abs</th>\n",
              "      <th>num_questions_rel</th>\n",
              "      <th>num_questions_abs</th>\n",
              "      <th>size_of_words_rel</th>\n",
              "      <th>size_of_words_abs</th>\n",
              "      <th>...</th>\n",
              "      <th>bert_nervousness</th>\n",
              "      <th>bert_optimism</th>\n",
              "      <th>bert_pride</th>\n",
              "      <th>bert_realization</th>\n",
              "      <th>bert_relief</th>\n",
              "      <th>bert_remorse</th>\n",
              "      <th>bert_sadness</th>\n",
              "      <th>bert_surprise</th>\n",
              "      <th>bert_neutral</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9fbb1cac6583f9df43d968c02cfc9eff</td>\n",
              "      <td>[CLS] Hey hiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii m/...</td>\n",
              "      <td>0.277439</td>\n",
              "      <td>565.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.280625</td>\n",
              "      <td>139.957143</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002213</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.007122</td>\n",
              "      <td>0.000705</td>\n",
              "      <td>0.001172</td>\n",
              "      <td>0.004991</td>\n",
              "      <td>0.008361</td>\n",
              "      <td>0.077860</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>afcdee26c7ba50454fe886e80eb16ded</td>\n",
              "      <td>[CLS] hi hi easter is gay how are u? yea agree...</td>\n",
              "      <td>0.572314</td>\n",
              "      <td>691.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.369517</td>\n",
              "      <td>148.843651</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002361</td>\n",
              "      <td>0.065858</td>\n",
              "      <td>0.004021</td>\n",
              "      <td>0.018808</td>\n",
              "      <td>0.002213</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.003822</td>\n",
              "      <td>0.002383</td>\n",
              "      <td>0.109519</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>aggresk8er-0</td>\n",
              "      <td>[CLS] hello how are you hi how are you goo goo...</td>\n",
              "      <td>0.026667</td>\n",
              "      <td>1184.0</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.260687</td>\n",
              "      <td>320.263490</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>0.003288</td>\n",
              "      <td>0.002020</td>\n",
              "      <td>0.007620</td>\n",
              "      <td>0.001647</td>\n",
              "      <td>0.001496</td>\n",
              "      <td>0.005680</td>\n",
              "      <td>0.004913</td>\n",
              "      <td>0.017312</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>aggresk8er-1</td>\n",
              "      <td>[CLS] Hello hi what ya doin? Nothing much just...</td>\n",
              "      <td>0.403315</td>\n",
              "      <td>289.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.255186</td>\n",
              "      <td>69.497619</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000915</td>\n",
              "      <td>0.004819</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.005034</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.015193</td>\n",
              "      <td>0.013506</td>\n",
              "      <td>0.002184</td>\n",
              "      <td>0.022046</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>aggresk8er-4</td>\n",
              "      <td>[CLS] Hello hi How r u doing good what u been ...</td>\n",
              "      <td>0.016981</td>\n",
              "      <td>1051.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.248014</td>\n",
              "      <td>221.249451</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.012490</td>\n",
              "      <td>0.003142</td>\n",
              "      <td>0.004357</td>\n",
              "      <td>0.002666</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.002366</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13154</th>\n",
              "      <td>13154</td>\n",
              "      <td>0</td>\n",
              "      <td>6377d2c072798e7ef0fd22f0769fdbae</td>\n",
              "      <td>[CLS] 15 m hey im 15 f from hey im bobby usa c...</td>\n",
              "      <td>0.131579</td>\n",
              "      <td>994.0</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.227453</td>\n",
              "      <td>221.526190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001099</td>\n",
              "      <td>0.007855</td>\n",
              "      <td>0.001591</td>\n",
              "      <td>0.011055</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.002752</td>\n",
              "      <td>0.002364</td>\n",
              "      <td>0.075391</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13155</th>\n",
              "      <td>13155</td>\n",
              "      <td>0</td>\n",
              "      <td>a2378441841247f277c54964860db8c9</td>\n",
              "      <td>[CLS] hiii hw r u hiiii are you okay ya u too ...</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>172.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.123126</td>\n",
              "      <td>68.850000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>0.002663</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.008098</td>\n",
              "      <td>0.000736</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.004075</td>\n",
              "      <td>0.000781</td>\n",
              "      <td>0.773637</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13156</th>\n",
              "      <td>13156</td>\n",
              "      <td>0</td>\n",
              "      <td>497ec625519d6e28dd360be81d01dbbd</td>\n",
              "      <td>[CLS] Why should interruptions be more problem...</td>\n",
              "      <td>0.811240</td>\n",
              "      <td>3067.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.808098</td>\n",
              "      <td>283.292332</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002674</td>\n",
              "      <td>0.010463</td>\n",
              "      <td>0.002572</td>\n",
              "      <td>0.024410</td>\n",
              "      <td>0.001482</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.005327</td>\n",
              "      <td>0.004805</td>\n",
              "      <td>0.375396</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13157</th>\n",
              "      <td>13157</td>\n",
              "      <td>0</td>\n",
              "      <td>dfbec532af50f894e0961bb5ef935c4c</td>\n",
              "      <td>[CLS] boo aaaaaaaaaah :o you scared me Yes I s...</td>\n",
              "      <td>0.060071</td>\n",
              "      <td>549.0</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.315303</td>\n",
              "      <td>137.597619</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006449</td>\n",
              "      <td>0.006909</td>\n",
              "      <td>0.005956</td>\n",
              "      <td>0.014781</td>\n",
              "      <td>0.020089</td>\n",
              "      <td>0.002330</td>\n",
              "      <td>0.009367</td>\n",
              "      <td>0.004675</td>\n",
              "      <td>0.026990</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13158</th>\n",
              "      <td>13158</td>\n",
              "      <td>0</td>\n",
              "      <td>57c30df4a8ebe316e03bb7d823de964f</td>\n",
              "      <td>[CLS] 21 m M or f Ok. F m age? Are looking for...</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>407.0</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.007870</td>\n",
              "      <td>116.025000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006828</td>\n",
              "      <td>0.010044</td>\n",
              "      <td>0.001704</td>\n",
              "      <td>0.028492</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.002017</td>\n",
              "      <td>0.009619</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>0.220681</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30574 rows × 54 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67f00449-3e62-4e6c-b2eb-a3619503ea9a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67f00449-3e62-4e6c-b2eb-a3619503ea9a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67f00449-3e62-4e6c-b2eb-a3619503ea9a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "print(\"loading csv files...\")\n",
        "X_train = pd.read_csv('/content/gdrive/MyDrive/PANC_datasets/PANC-train_add_features_emotion.csv', header=0, encoding='latin1')\n",
        "X_test = pd.read_csv('/content/gdrive/MyDrive/PANC_datasets/PANC-test_add_features_emotion.csv', header=0, encoding= 'latin1')\n",
        "\n",
        "\n",
        "\n",
        "#just for inspection and normalization\n",
        "# Add a column named 'split', and specify which split each row belongs to.\n",
        "X_train['split'] = 'train'\n",
        "X_test['split'] = 'test'\n",
        "\n",
        "\n",
        "# Combine the two into a single dataframe for processing.\n",
        "df = pd.concat([X_train, X_test])\n",
        "#transform to numerical labels\n",
        "df.loc[:, 'label'] = (df['label'] == 'predator').apply(int)\n",
        "df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#specify our numerical features\n",
        "all_columns = df.columns.values.tolist()\n",
        "\n",
        "print(all_columns)\n",
        "\n",
        "col_info = {}\n",
        "col_info['text_cols'] = [\n",
        "                         \"chatName\",\n",
        "                         \"segment\",\n",
        "]\n",
        "col_info['num_cols'] = [\n",
        "             \"length_rel\",\n",
        "             \"length_abs\",\n",
        "             \"num_questions_rel\",\n",
        "             \"num_questions_abs\",\n",
        "             \"size_of_words_rel\",\n",
        "             \"size_of_words_abs\",\n",
        "             'empath_help', 'empath_office', 'empath_dance', 'empath_money', 'empath_wedding', 'empath_domestic_work', 'empath_sleep', 'empath_medical_emergency', 'empath_cold', 'empath_hate', 'empath_cheerfulness', 'empath_aggression', 'empath_occupation', 'empath_envy', 'empath_anticipation', \n",
        "             'bert_admiration', 'bert_amusement', 'bert_anger', 'bert_annoyance', 'bert_approval', 'bert_caring', 'bert_confusion', 'bert_curiosity', 'bert_desire', 'bert_disappointment', 'bert_disapproval', 'bert_disgust', 'bert_embarrassment', 'bert_excitement', 'bert_fear', 'bert_gratitude', 'bert_grief', 'bert_joy', 'bert_love', 'bert_nervousness', 'bert_optimism', 'bert_pride', 'bert_realization', 'bert_relief', 'bert_remorse', 'bert_sadness', 'bert_surprise', 'bert_neutral',\n",
        "            ]\n",
        "\n",
        "\n",
        "print('There are:')\n",
        "print('  {:,}  numerical features'.format(len(col_info['num_cols'])))\n",
        "\n",
        "# Select just the columns we're using.\n",
        "data_df = df[col_info['text_cols'] +  \n",
        "             col_info['num_cols'] +\n",
        "             ['label', 'split']] # 'label' is our label.\n",
        "              \n",
        "\n",
        "data_df.head()\n",
        "data_df = data_df.fillna(\"\")"
      ],
      "metadata": {
        "id": "z00R5j0gI28j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577a2152-ca7a-4dce-d711-3cc5239fe5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unnamed: 0', 'label', 'chatName', 'segment', 'length_rel', 'length_abs', 'num_questions_rel', 'num_questions_abs', 'size_of_words_rel', 'size_of_words_abs', 'empath_help', 'empath_office', 'empath_dance', 'empath_money', 'empath_wedding', 'empath_domestic_work', 'empath_sleep', 'empath_medical_emergency', 'empath_cold', 'empath_hate', 'empath_cheerfulness', 'empath_aggression', 'empath_occupation', 'empath_envy', 'empath_anticipation', 'bert_admiration', 'bert_amusement', 'bert_anger', 'bert_annoyance', 'bert_approval', 'bert_caring', 'bert_confusion', 'bert_curiosity', 'bert_desire', 'bert_disappointment', 'bert_disapproval', 'bert_disgust', 'bert_embarrassment', 'bert_excitement', 'bert_fear', 'bert_gratitude', 'bert_grief', 'bert_joy', 'bert_love', 'bert_nervousness', 'bert_optimism', 'bert_pride', 'bert_realization', 'bert_relief', 'bert_remorse', 'bert_sadness', 'bert_surprise', 'bert_neutral', 'split']\n",
            "There are:\n",
            "  49  numerical features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[col_info['num_cols']].head()"
      ],
      "metadata": {
        "id": "LGhWcCryKtSn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "2462bf20-aab0-4c2a-fdd8-b95f4d07f121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  length_rel length_abs num_questions_rel num_questions_abs size_of_words_rel  \\\n",
              "0   0.277439      565.0               1.0               3.0          0.280625   \n",
              "1   0.572314      691.0               0.0               4.0          0.369517   \n",
              "2   0.026667     1184.0          0.857143              16.0          0.260687   \n",
              "3   0.403315      289.0               0.5               3.0          0.255186   \n",
              "4   0.016981     1051.0               1.0               9.0          0.248014   \n",
              "\n",
              "  size_of_words_abs empath_help empath_office empath_dance empath_money  ...  \\\n",
              "0        139.957143         0.0           0.0          0.0          0.0  ...   \n",
              "1        148.843651         0.0           0.0          0.0          0.0  ...   \n",
              "2         320.26349         0.0           0.0          1.0          0.0  ...   \n",
              "3         69.497619         1.0           0.0          0.0          0.0  ...   \n",
              "4        221.249451         0.0           0.0          1.0          0.0  ...   \n",
              "\n",
              "  bert_love bert_nervousness bert_optimism bert_pride bert_realization  \\\n",
              "0  0.000419         0.002213      0.002550   0.000968         0.007122   \n",
              "1  0.002123         0.002361      0.065858   0.004021         0.018808   \n",
              "2  0.002156         0.000863      0.003288   0.002020         0.007620   \n",
              "3  0.001936         0.000915      0.004819   0.000722         0.005034   \n",
              "4  0.001744         0.000613      0.012490   0.003142         0.004357   \n",
              "\n",
              "  bert_relief bert_remorse bert_sadness bert_surprise bert_neutral  \n",
              "0    0.000705     0.001172     0.004991      0.008361     0.077860  \n",
              "1    0.002213     0.001050     0.003822      0.002383     0.109519  \n",
              "2    0.001647     0.001496     0.005680      0.004913     0.017312  \n",
              "3    0.000704     0.015193     0.013506      0.002184     0.022046  \n",
              "4    0.002666     0.000648     0.001321      0.002366     0.013954  \n",
              "\n",
              "[5 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25e0002a-324e-42b3-b69b-a6af3149e8d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>length_rel</th>\n",
              "      <th>length_abs</th>\n",
              "      <th>num_questions_rel</th>\n",
              "      <th>num_questions_abs</th>\n",
              "      <th>size_of_words_rel</th>\n",
              "      <th>size_of_words_abs</th>\n",
              "      <th>empath_help</th>\n",
              "      <th>empath_office</th>\n",
              "      <th>empath_dance</th>\n",
              "      <th>empath_money</th>\n",
              "      <th>...</th>\n",
              "      <th>bert_love</th>\n",
              "      <th>bert_nervousness</th>\n",
              "      <th>bert_optimism</th>\n",
              "      <th>bert_pride</th>\n",
              "      <th>bert_realization</th>\n",
              "      <th>bert_relief</th>\n",
              "      <th>bert_remorse</th>\n",
              "      <th>bert_sadness</th>\n",
              "      <th>bert_surprise</th>\n",
              "      <th>bert_neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.277439</td>\n",
              "      <td>565.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.280625</td>\n",
              "      <td>139.957143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000419</td>\n",
              "      <td>0.002213</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.007122</td>\n",
              "      <td>0.000705</td>\n",
              "      <td>0.001172</td>\n",
              "      <td>0.004991</td>\n",
              "      <td>0.008361</td>\n",
              "      <td>0.077860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.572314</td>\n",
              "      <td>691.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.369517</td>\n",
              "      <td>148.843651</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002123</td>\n",
              "      <td>0.002361</td>\n",
              "      <td>0.065858</td>\n",
              "      <td>0.004021</td>\n",
              "      <td>0.018808</td>\n",
              "      <td>0.002213</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.003822</td>\n",
              "      <td>0.002383</td>\n",
              "      <td>0.109519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.026667</td>\n",
              "      <td>1184.0</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.260687</td>\n",
              "      <td>320.26349</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002156</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>0.003288</td>\n",
              "      <td>0.002020</td>\n",
              "      <td>0.007620</td>\n",
              "      <td>0.001647</td>\n",
              "      <td>0.001496</td>\n",
              "      <td>0.005680</td>\n",
              "      <td>0.004913</td>\n",
              "      <td>0.017312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.403315</td>\n",
              "      <td>289.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.255186</td>\n",
              "      <td>69.497619</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001936</td>\n",
              "      <td>0.000915</td>\n",
              "      <td>0.004819</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.005034</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.015193</td>\n",
              "      <td>0.013506</td>\n",
              "      <td>0.002184</td>\n",
              "      <td>0.022046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.016981</td>\n",
              "      <td>1051.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.248014</td>\n",
              "      <td>221.249451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001744</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.012490</td>\n",
              "      <td>0.003142</td>\n",
              "      <td>0.004357</td>\n",
              "      <td>0.002666</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.002366</td>\n",
              "      <td>0.013954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 49 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25e0002a-324e-42b3-b69b-a6af3149e8d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-25e0002a-324e-42b3-b69b-a6af3149e8d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-25e0002a-324e-42b3-b69b-a6af3149e8d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize feature distribution to help MLP\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "for x in col_info['num_cols']:\n",
        "    data_df[x].replace('', 0, inplace=True)\n",
        "    col_val = data_df[x].values.reshape(-1, 1)\n",
        "\n",
        "    numerical_transformer = QuantileTransformer(output_distribution='normal')\n",
        "    \n",
        "\n",
        "    data_df[x] = numerical_transformer.fit_transform(col_val)\n",
        "\n",
        "    \n",
        "data_df[col_info['num_cols']].head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "yZZCdymdIYkL",
        "outputId": "eb43486e-a4a1-4cbd-d844-ff95cf211815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   length_rel  length_abs  num_questions_rel  num_questions_abs  \\\n",
              "0   -0.341501    0.139710           5.199338          -0.634851   \n",
              "1    0.665623    0.352151          -5.199338          -0.427976   \n",
              "2   -1.745266    0.894843           0.494971           1.081940   \n",
              "3    0.097195   -0.697507          -0.204660          -0.634851   \n",
              "4   -1.887577    0.773986           5.199338           0.388430   \n",
              "\n",
              "   size_of_words_rel  size_of_words_abs  empath_help  empath_office  \\\n",
              "0          -0.078946           0.250185    -5.199338      -5.199338   \n",
              "1           0.252356           0.330791    -5.199338      -5.199338   \n",
              "2          -0.157644           1.233666    -5.199338      -5.199338   \n",
              "3          -0.179156          -1.013596     5.199338      -5.199338   \n",
              "4          -0.206617           0.808818    -5.199338      -5.199338   \n",
              "\n",
              "   empath_dance  empath_money  ...  bert_love  bert_nervousness  \\\n",
              "0     -5.199338     -5.199338  ...  -1.905080          0.449730   \n",
              "1     -5.199338     -5.199338  ...   0.126973          0.528085   \n",
              "2      5.199338     -5.199338  ...   0.143349         -0.848026   \n",
              "3     -5.199338     -5.199338  ...   0.030505         -0.755930   \n",
              "4      5.199338     -5.199338  ...  -0.083118         -1.431763   \n",
              "\n",
              "   bert_optimism  bert_pride  bert_realization  bert_relief  bert_remorse  \\\n",
              "0      -1.614523   -0.902874         -0.978498    -0.702830      0.355550   \n",
              "1       1.859563    0.944741          0.589005     0.686556      0.217900   \n",
              "2      -1.217205    0.064464         -0.874907     0.386868      0.606537   \n",
              "3      -0.603514   -1.348888         -1.569547    -0.705494      1.656367   \n",
              "4       0.672838    0.620307         -1.800414     0.851644     -0.543360   \n",
              "\n",
              "   bert_sadness  bert_surprise  bert_neutral  \n",
              "0      0.458676       0.670574     -0.322039  \n",
              "1      0.191087      -0.914693     -0.089193  \n",
              "2      0.575667       0.062123     -1.434942  \n",
              "3      1.144054      -1.041340     -1.242587  \n",
              "4     -1.361949      -0.924818     -1.621891  \n",
              "\n",
              "[5 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-158e4f8e-46d1-4eca-b4a5-775d16525077\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>length_rel</th>\n",
              "      <th>length_abs</th>\n",
              "      <th>num_questions_rel</th>\n",
              "      <th>num_questions_abs</th>\n",
              "      <th>size_of_words_rel</th>\n",
              "      <th>size_of_words_abs</th>\n",
              "      <th>empath_help</th>\n",
              "      <th>empath_office</th>\n",
              "      <th>empath_dance</th>\n",
              "      <th>empath_money</th>\n",
              "      <th>...</th>\n",
              "      <th>bert_love</th>\n",
              "      <th>bert_nervousness</th>\n",
              "      <th>bert_optimism</th>\n",
              "      <th>bert_pride</th>\n",
              "      <th>bert_realization</th>\n",
              "      <th>bert_relief</th>\n",
              "      <th>bert_remorse</th>\n",
              "      <th>bert_sadness</th>\n",
              "      <th>bert_surprise</th>\n",
              "      <th>bert_neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.341501</td>\n",
              "      <td>0.139710</td>\n",
              "      <td>5.199338</td>\n",
              "      <td>-0.634851</td>\n",
              "      <td>-0.078946</td>\n",
              "      <td>0.250185</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.905080</td>\n",
              "      <td>0.449730</td>\n",
              "      <td>-1.614523</td>\n",
              "      <td>-0.902874</td>\n",
              "      <td>-0.978498</td>\n",
              "      <td>-0.702830</td>\n",
              "      <td>0.355550</td>\n",
              "      <td>0.458676</td>\n",
              "      <td>0.670574</td>\n",
              "      <td>-0.322039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.665623</td>\n",
              "      <td>0.352151</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-0.427976</td>\n",
              "      <td>0.252356</td>\n",
              "      <td>0.330791</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126973</td>\n",
              "      <td>0.528085</td>\n",
              "      <td>1.859563</td>\n",
              "      <td>0.944741</td>\n",
              "      <td>0.589005</td>\n",
              "      <td>0.686556</td>\n",
              "      <td>0.217900</td>\n",
              "      <td>0.191087</td>\n",
              "      <td>-0.914693</td>\n",
              "      <td>-0.089193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.745266</td>\n",
              "      <td>0.894843</td>\n",
              "      <td>0.494971</td>\n",
              "      <td>1.081940</td>\n",
              "      <td>-0.157644</td>\n",
              "      <td>1.233666</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>...</td>\n",
              "      <td>0.143349</td>\n",
              "      <td>-0.848026</td>\n",
              "      <td>-1.217205</td>\n",
              "      <td>0.064464</td>\n",
              "      <td>-0.874907</td>\n",
              "      <td>0.386868</td>\n",
              "      <td>0.606537</td>\n",
              "      <td>0.575667</td>\n",
              "      <td>0.062123</td>\n",
              "      <td>-1.434942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.097195</td>\n",
              "      <td>-0.697507</td>\n",
              "      <td>-0.204660</td>\n",
              "      <td>-0.634851</td>\n",
              "      <td>-0.179156</td>\n",
              "      <td>-1.013596</td>\n",
              "      <td>5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030505</td>\n",
              "      <td>-0.755930</td>\n",
              "      <td>-0.603514</td>\n",
              "      <td>-1.348888</td>\n",
              "      <td>-1.569547</td>\n",
              "      <td>-0.705494</td>\n",
              "      <td>1.656367</td>\n",
              "      <td>1.144054</td>\n",
              "      <td>-1.041340</td>\n",
              "      <td>-1.242587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.887577</td>\n",
              "      <td>0.773986</td>\n",
              "      <td>5.199338</td>\n",
              "      <td>0.388430</td>\n",
              "      <td>-0.206617</td>\n",
              "      <td>0.808818</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>5.199338</td>\n",
              "      <td>-5.199338</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.083118</td>\n",
              "      <td>-1.431763</td>\n",
              "      <td>0.672838</td>\n",
              "      <td>0.620307</td>\n",
              "      <td>-1.800414</td>\n",
              "      <td>0.851644</td>\n",
              "      <td>-0.543360</td>\n",
              "      <td>-1.361949</td>\n",
              "      <td>-0.924818</td>\n",
              "      <td>-1.621891</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 49 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-158e4f8e-46d1-4eca-b4a5-775d16525077')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-158e4f8e-46d1-4eca-b4a5-775d16525077 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-158e4f8e-46d1-4eca-b4a5-775d16525077');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-split the dataset.\n",
        "test_df = data_df.loc[data_df.split == 'test']\n",
        "train_df = data_df.loc[data_df.split == 'train']\n",
        "\n",
        "datasets = {'train': train_df, 'eval': test_df}\n"
      ],
      "metadata": {
        "id": "q9xUJNG7MSa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions: MLP\n",
        "\n",
        "This is a standard MLP, which is used on the CLS embeddings of BERT's output to generate our final predictions with the feature vectors. \n",
        "The MLP was taken directly from the Multimodal-Toolkit [here](https://github.com/georgian-io/Multimodal-Toolkit/blob/master/multimodal_transformers/model/layer_utils.py)\n"
      ],
      "metadata": {
        "id": "-bcjKtHSXUEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"mlp can specify number of hidden layers and hidden layer channels\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, num_hidden_lyr=2,\n",
        "                 dropout_prob=0.5, return_layer_outs=False,\n",
        "                 hidden_channels=None, bn=False):\n",
        "        super().__init__()\n",
        "        self.out_dim = output_dim\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.return_layer_outs = return_layer_outs\n",
        "        if not hidden_channels:\n",
        "            hidden_channels = [input_dim for _ in range(num_hidden_lyr)]\n",
        "        elif len(hidden_channels) != num_hidden_lyr:\n",
        "            raise ValueError(\n",
        "                \"number of hidden layers should be the same as the lengh of hidden_channels\")\n",
        "        self.layer_channels = [input_dim] + hidden_channels + [output_dim]\n",
        "        self.act_name = 'relu'\n",
        "        self.activation = nn.ReLU()\n",
        "        self.layers = nn.ModuleList(list(\n",
        "            map(self.weight_init, [nn.Linear(self.layer_channels[i], self.layer_channels[i + 1])\n",
        "                                   for i in range(len(self.layer_channels) - 2)])))\n",
        "        final_layer = nn.Linear(self.layer_channels[-2], self.layer_channels[-1])\n",
        "        self.weight_init(final_layer,   activation='linear')\n",
        "        self.layers.append(final_layer)\n",
        "\n",
        "        self.bn = bn\n",
        "        if self.bn:\n",
        "            self.bn = nn.ModuleList([torch.nn.BatchNorm1d(dim) for dim in self.layer_channels[1:-1]])\n",
        "\n",
        "    def weight_init(self, m, activation=None):\n",
        "        if activation is None:\n",
        "            activation = self.act_name\n",
        "        torch.nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain(activation))\n",
        "        return m\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: the input features\n",
        "        :return: tuple containing output of MLP,\n",
        "                and list of inputs and outputs at every layer\n",
        "        \"\"\"\n",
        "        layer_inputs = [x]\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            input = layer_inputs[-1]\n",
        "            if layer == self.layers[-1]:\n",
        "                layer_inputs.append(layer(input))\n",
        "            else:\n",
        "                if self.bn:\n",
        "                    output = self.activation(self.bn[i](layer(input)))\n",
        "                else:\n",
        "                    output = self.activation(layer(input))\n",
        "                layer_inputs.append(self.dropout(output))\n",
        "\n",
        "        # model.store_layer_output(self, layer_inputs[-1])\n",
        "        if self.return_layer_outs:\n",
        "            return layer_inputs[-1], layer_inputs\n",
        "        else:\n",
        "            return layer_inputs[-1]"
      ],
      "metadata": {
        "id": "vYMiIndXYpvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Precalculate dimensions of MLP (numbers are manually added in the Modified BERT class)\n",
        "# According to the formula in the MultiModal-Toolkit, each layer of the MLP has 1/4th the number of neurons as the previous one. \n",
        "dims = []\n",
        "dim = 768 + 49 #hidden size of bert cls + number of features\n",
        "# Keep dividing by 4\n",
        "while True:\n",
        "    # Divide by 4 and truncate to an integer.\n",
        "    dim = dim // 4\n",
        "    \n",
        "    # until resulting layer is smaller than #outputs. Since we have a classification task, that's 2\n",
        "    if dim <= 2:\n",
        "        break\n",
        "    # store as next layer\n",
        "    dims.append(int(dim))\n",
        "\n",
        "# Print out the resulting MLP.                \n",
        "print('MLP layer sizes:')\n",
        "print('Input:', dim)\n",
        "print('Hidden:', dims)\n",
        "print('Output:', 2)\n",
        "\n",
        "#Results: Input: 817\n",
        "          #Hidden: [204, 51, 12, 3]\n",
        "          #Output: 2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tfpMrUwjlt7",
        "outputId": "745a627d-988f-4508-8d98-558f66f78b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP layer sizes:\n",
            "Input: 0\n",
            "Hidden: [204, 51, 12, 3]\n",
            "Output: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions: Modified BERT\n",
        "\n",
        "This class modifies the forward function of BertForSequenceClassification in order to pass the CLS embeddings of the output + the feature vector through an MLP"
      ],
      "metadata": {
        "id": "3QJbBbgMXosQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXTbhaQ_oTem"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "\n",
        "class BertModified(BertForSequenceClassification):\n",
        "    \"\"\"\n",
        "    Uses a transformers.BertConfig object which has two additional properties manually added to it:\n",
        "      `text_feat_dim` - The length of the BERT vector (usually 768)\n",
        "      `numerical_feat_dim` - The number of numerical features (in our case 49)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        # Calling constructor of the`BertForSequenceClassification` class, does all of the BERT-related setup. Resulting BERT model is stored in `self.bert`\n",
        "        super().__init__(config)\n",
        "        # Create a batch normalizer for the numerical features.\n",
        "        # Calculate the combined vector length. \n",
        "        combined_feat_dim = config.text_feat_dim + config.numerical_feat_dim\n",
        "        self.num_bn = nn.BatchNorm1d(config.numerical_feat_dim)\n",
        "        #As previously calculated, specify the number of inputs, outputs, and the\n",
        "        # layer sizes when constructing the MLP.\n",
        "        self.mlp = MLP(combined_feat_dim, #text features dim + numerical features dim\n",
        "                        2, #number of labels\n",
        "                        num_hidden_lyr=4, #len(dims)\n",
        "                        dropout_prob=0.1,\n",
        "                        hidden_channels= [204, 51, 12, 3], #dims\n",
        "                        bn=True)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        class_weights=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        numerical_feats=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        forward pass of our model with the same inputs as `forward` in `BertForSequenceClassification`\n",
        "        + one extra parameter: `numerical_feats` = Tensor of numerical features.  \n",
        "        \"\"\"\n",
        "        #`self.bert` returns outputs from the encoding layers, and not from the final classifier.\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "        \n",
        "        # outputs[0] - All of the output embeddings from BERT\n",
        "        # outputs[1] - Apparently The [CLS] token embedding\n",
        "        cls = outputs[1] #should be of size [batch size  x   768]\n",
        "       # Apply batch normalizat ion to the numerical features.        \n",
        "        numerical_feats = self.num_bn(numerical_feats) #should be of size [batch size  x   # numerical features]\n",
        "\n",
        "        #concatenate into one vector\n",
        "        combined_feats = torch.cat((cls, numerical_feats),\n",
        "                                    dim=1)\n",
        "     \n",
        "        # Run the the samples through the MLP.\n",
        "        logits = self.mlp(combined_feats)\n",
        "\n",
        "        if type(logits) is tuple:\n",
        "            logits = logits[0]\n",
        "\n",
        "        # If we're doing training or validation, calculate loss with the provided labels\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss(weight=class_weights)\n",
        "            labels = labels.long()\n",
        "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "        \n",
        "        # During test time, we don't need labels.\n",
        "        else:\n",
        "            loss = None\n",
        "        \n",
        "        # Put the results into a Dictionary to return.\n",
        "        results = {'loss': loss, \n",
        "                   'logits': logits}\n",
        "\n",
        "        return results\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training class"
      ],
      "metadata": {
        "id": "L2mDbs4EX1Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "import torch\n",
        "\n",
        "class ESPD:\n",
        "    '''\n",
        "    Implementation of the Early Sexual Predator Detection paper\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "    \n",
        "        self.config = BertConfig.from_pretrained(\"bert-base-cased\",\n",
        "                                    output_hidden_states=True, num_labels = 2) #for classification\n",
        "        self.num_epochs = 4\n",
        "        self.training_stats = []\n",
        "\n",
        "    def tokenize_function(self, text_list):\n",
        "        print('Encoding {:,} text samples...'.format(len(text_list)))\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "        for row in text_list:\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                row,                  # Sentence to encode.\n",
        "                                truncation = True,\n",
        "                                padding = 'max_length',\n",
        "                                pad_to_max_length = True, # Pad & truncate all sentences.\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        print(\"done\")\n",
        "        return input_ids, attention_masks\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return self.metric.compute(predictions=predictions, references=labels)\n",
        "    \n",
        "\n",
        "    def split(self, dataset):\n",
        "        # Create a 90-10 train-validation split.\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def create_tensorData(self, input_ids_t, attention_masks_t, features_t, labels_t):\n",
        "\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids_t, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks_t, dim=0)\n",
        "        labels = torch.tensor(labels_t.values)\n",
        "        numerical_feats = torch.tensor(features_t.values.astype('float'), dtype=torch.float32)\n",
        "        #finish setting up Config and the actual model\n",
        "        self.config.numerical_feat_dim = numerical_feats.size()[1]\n",
        "\n",
        "        data = TensorDataset(input_ids, attention_masks, labels, numerical_feats)\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def preprocess_dataset(self, dataset):\n",
        "        texts_train, labels_train, features_train = dataset['train']['segment'], dataset['train']['label'], dataset['train'][col_info['num_cols']]\n",
        "        texts_test, labels_test, features_test =  dataset['eval']['segment'], dataset['eval']['label'], dataset['eval'][col_info['num_cols']]\n",
        "\n",
        "\n",
        "        texts_train.map(lambda x: x.replace('[CLS] ', ''))\n",
        "        texts_test.map(lambda x: x.replace('[CLS] ', ''))\n",
        "\n",
        "        input_ids_train, attention_masks_train = self.tokenize_function(texts_train.tolist())\n",
        "        input_ids_test, attention_masks_test = self.tokenize_function(texts_test.tolist())\n",
        "\n",
        "        train_and_val_data = self.create_tensorData(input_ids_train, attention_masks_train, features_train, labels_train)\n",
        "        test_dataset = self.create_tensorData(input_ids_test, attention_masks_test, features_test, labels_test)\n",
        "\n",
        "        #split into train and validation set\n",
        "        print(\"creating a validation set out of training data...\")\n",
        "        train_dataset, val_dataset = self.split(train_and_val_data)\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            shuffle=True, # Select batches randomly\n",
        "            batch_size = 16 # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        validation_dataloader = DataLoader(\n",
        "                    val_dataset, # The validation samples.\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                    batch_size = 16 # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "        test_dataloader = DataLoader(\n",
        "            test_dataset, \n",
        "            sampler=SequentialSampler(test_dataset), \n",
        "            batch_size=16)\n",
        "\n",
        "        \n",
        "        #finish setting up the model\n",
        "        self.config.text_feat_dim = self.config.hidden_size # 768\n",
        "        self.model = BertModified.from_pretrained(\"bert-base-cased\", config = self.config)\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        desc = self.model.cuda()\n",
        "\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr = 3e-3, eps = 1e-8)\n",
        "        self.num_training_steps = self.num_epochs * len(train_dataloader)\n",
        "        self.lr_scheduler = get_scheduler(name=\"linear\", optimizer=self.optimizer, num_warmup_steps=0, num_training_steps=self.num_training_steps)\n",
        "\n",
        "        print(\"returning dataloaders...\")\n",
        "\n",
        "        return train_dataloader, validation_dataloader, test_dataloader\n",
        "\n",
        "\n",
        "    def start_training(self, train_dataloader, validation_dataloader):\n",
        "          for epoch_i in range(0, self.num_epochs):\n",
        "                    print(\"\")\n",
        "                    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, self.num_epochs))\n",
        "                    print('Training...')\n",
        "                    # Reset the total loss for this epoch.\n",
        "                    total_train_loss = 0\n",
        "                    #set model in training mode (doesn't train yet, is just a mode)\n",
        "                    self.model.train()\n",
        "                    for i, batch in enumerate(train_dataloader):\n",
        "                          if i % 40 == 0 and not i == 0:\n",
        "                            print('  Batch {:>5,}  of  {:>5,}.'.format(i, len(train_dataloader)))\n",
        "                          #Unpack this training batch from our dataloader and copy each tensor to device\n",
        "                          b_input_ids = batch[0].to(device)\n",
        "                          b_input_mask = batch[1].to(device)\n",
        "                          b_labels = batch[2].to(device)\n",
        "                          b_numerical_feats = batch[3].to(device)\n",
        "\n",
        "                          #clear any previously calculated gradients before performing backward pass\n",
        "                          self.model.zero_grad()\n",
        "\n",
        "                          #forward pass\n",
        "                          result = self.model(b_input_ids,\n",
        "                                      token_type_ids=None,\n",
        "                                      attention_mask=b_input_mask,\n",
        "                                      labels=b_labels,\n",
        "                                      numerical_feats = b_numerical_feats)\n",
        "\n",
        "                          loss = result['loss']\n",
        "\n",
        "                          total_train_loss += loss.item() #returns python value from tensor\n",
        "\n",
        "                          #backward pass\n",
        "                          loss.backward()\n",
        "\n",
        "                          #clip norm of gradients to 1 so that they dont explode\n",
        "                          torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                          #update rule\n",
        "                          self.optimizer.step()\n",
        "                          # Update the learning rate.\n",
        "                          self.lr_scheduler.step()\n",
        "                    \n",
        "                    avg_train_loss = total_train_loss / (len(train_dataloader) * 16)\n",
        "                    print(\"\")\n",
        "                    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "                    print(\"\")\n",
        "                    print(\"Running Validation...\")\n",
        "                    #change mode of evaluation\n",
        "                    self.model.eval()\n",
        "                    total_eval_loss = 0\n",
        "                    for i, batch in enumerate(validation_dataloader):\n",
        "                            if i % 40 == 0 and not i == 0:\n",
        "                              print('  Batch {:>5,}  of  {:>5,}.'.format(i, len(validation_dataloader)))\n",
        "                            #Unpack this training batch from our dataloader and copy each tensor to device\n",
        "                            b_input_ids = batch[0].to(device)\n",
        "                            b_input_mask = batch[1].to(device)\n",
        "                            b_labels = batch[2].to(device)\n",
        "                            b_numerical_feats = batch[3].to(device)\n",
        "          \n",
        "                            #since this is validation\n",
        "                            with torch.no_grad():\n",
        "                              #forward pass\n",
        "                              result = self.model(b_input_ids,\n",
        "                                            token_type_ids=None,\n",
        "                                            attention_mask=b_input_mask,\n",
        "                                            labels=b_labels,\n",
        "                                            numerical_feats = b_numerical_feats)\n",
        "          \n",
        "                              loss = result['loss']\n",
        "                        \n",
        "                              total_eval_loss += loss.item()\n",
        "                          \n",
        "                      # Calculate the average loss over all of the batches.\n",
        "                    avg_val_loss = total_eval_loss / (len(validation_dataloader) * 16)\n",
        "                    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "                    # Record all statistics from this epoch.\n",
        "                    self.training_stats.append(\n",
        "                            {\n",
        "                              'epoch': epoch_i + 1,\n",
        "                              'Training Loss': avg_train_loss,\n",
        "                              'Valid. Loss': avg_val_loss\n",
        "                            })\n",
        "          torch.save(self.model, '/content/gdrive/MyDrive/mzModel2')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "TYoVMCRIR_3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Calls to classes"
      ],
      "metadata": {
        "id": "EQkVxuoxbeld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eSPD = ESPD()\n",
        "train_dataloader, validation_dataloader, test_dataloader = eSPD.preprocess_dataset(datasets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtdlnmNARF5t",
        "outputId": "2ac53840-f926-4f2c-987a-7ee57dee8b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 17,415 text samples...\n",
            "done\n",
            "Encoding 13,159 text samples...\n",
            "done\n",
            "creating a validation set out of training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModified: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModified from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModified from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModified were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['num_bn.weight', 'mlp.layers.0.bias', 'mlp.bn.1.running_mean', 'mlp.bn.1.running_var', 'mlp.bn.1.bias', 'mlp.bn.3.running_mean', 'mlp.bn.3.running_var', 'mlp.bn.3.weight', 'num_bn.bias', 'mlp.bn.2.bias', 'classifier.bias', 'mlp.bn.2.weight', 'num_bn.running_var', 'mlp.bn.3.num_batches_tracked', 'mlp.layers.1.weight', 'num_bn.num_batches_tracked', 'mlp.bn.2.running_var', 'mlp.bn.3.bias', 'mlp.bn.0.num_batches_tracked', 'mlp.layers.4.bias', 'mlp.layers.2.bias', 'mlp.bn.0.running_mean', 'mlp.layers.4.weight', 'mlp.layers.3.bias', 'classifier.weight', 'mlp.bn.0.bias', 'mlp.bn.2.num_batches_tracked', 'mlp.layers.2.weight', 'mlp.bn.1.num_batches_tracked', 'mlp.bn.0.running_var', 'mlp.bn.0.weight', 'mlp.layers.1.bias', 'mlp.bn.2.running_mean', 'mlp.bn.1.weight', 'mlp.layers.3.weight', 'num_bn.running_mean', 'mlp.layers.0.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "returning dataloaders...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "#for reproducability\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "eSPD.start_training(train_dataloader, validation_dataloader)"
      ],
      "metadata": {
        "id": "Qp1Jl2Pqh9ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05136c7c-71f7-452e-fde8-833d6d768a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    980.\n",
            "  Batch    80  of    980.\n",
            "  Batch   120  of    980.\n",
            "  Batch   160  of    980.\n",
            "  Batch   200  of    980.\n",
            "  Batch   240  of    980.\n",
            "  Batch   280  of    980.\n",
            "  Batch   320  of    980.\n",
            "  Batch   360  of    980.\n",
            "  Batch   400  of    980.\n",
            "  Batch   440  of    980.\n",
            "  Batch   480  of    980.\n",
            "  Batch   520  of    980.\n",
            "  Batch   560  of    980.\n",
            "  Batch   600  of    980.\n",
            "  Batch   640  of    980.\n",
            "  Batch   680  of    980.\n",
            "  Batch   720  of    980.\n",
            "  Batch   760  of    980.\n",
            "  Batch   800  of    980.\n",
            "  Batch   840  of    980.\n",
            "  Batch   880  of    980.\n",
            "  Batch   920  of    980.\n",
            "  Batch   960  of    980.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "\n",
            "Running Validation...\n",
            "  Batch    40  of    109.\n",
            "  Batch    80  of    109.\n",
            "  Validation Loss: 0.01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    980.\n",
            "  Batch    80  of    980.\n",
            "  Batch   120  of    980.\n",
            "  Batch   160  of    980.\n",
            "  Batch   200  of    980.\n",
            "  Batch   240  of    980.\n",
            "  Batch   280  of    980.\n",
            "  Batch   320  of    980.\n",
            "  Batch   360  of    980.\n",
            "  Batch   400  of    980.\n",
            "  Batch   440  of    980.\n",
            "  Batch   480  of    980.\n",
            "  Batch   520  of    980.\n",
            "  Batch   560  of    980.\n",
            "  Batch   600  of    980.\n",
            "  Batch   640  of    980.\n",
            "  Batch   680  of    980.\n",
            "  Batch   720  of    980.\n",
            "  Batch   760  of    980.\n",
            "  Batch   800  of    980.\n",
            "  Batch   840  of    980.\n",
            "  Batch   880  of    980.\n",
            "  Batch   920  of    980.\n",
            "  Batch   960  of    980.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "\n",
            "Running Validation...\n",
            "  Batch    40  of    109.\n",
            "  Batch    80  of    109.\n",
            "  Validation Loss: 0.01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    980.\n",
            "  Batch    80  of    980.\n",
            "  Batch   120  of    980.\n",
            "  Batch   160  of    980.\n",
            "  Batch   200  of    980.\n",
            "  Batch   240  of    980.\n",
            "  Batch   280  of    980.\n",
            "  Batch   320  of    980.\n",
            "  Batch   360  of    980.\n",
            "  Batch   400  of    980.\n",
            "  Batch   440  of    980.\n",
            "  Batch   480  of    980.\n",
            "  Batch   520  of    980.\n",
            "  Batch   560  of    980.\n",
            "  Batch   600  of    980.\n",
            "  Batch   640  of    980.\n",
            "  Batch   680  of    980.\n",
            "  Batch   720  of    980.\n",
            "  Batch   760  of    980.\n",
            "  Batch   800  of    980.\n",
            "  Batch   840  of    980.\n",
            "  Batch   880  of    980.\n",
            "  Batch   920  of    980.\n",
            "  Batch   960  of    980.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "\n",
            "Running Validation...\n",
            "  Batch    40  of    109.\n",
            "  Batch    80  of    109.\n",
            "  Validation Loss: 0.01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    980.\n",
            "  Batch    80  of    980.\n",
            "  Batch   120  of    980.\n",
            "  Batch   160  of    980.\n",
            "  Batch   200  of    980.\n",
            "  Batch   240  of    980.\n",
            "  Batch   280  of    980.\n",
            "  Batch   320  of    980.\n",
            "  Batch   360  of    980.\n",
            "  Batch   400  of    980.\n",
            "  Batch   440  of    980.\n",
            "  Batch   480  of    980.\n",
            "  Batch   520  of    980.\n",
            "  Batch   560  of    980.\n",
            "  Batch   600  of    980.\n",
            "  Batch   640  of    980.\n",
            "  Batch   680  of    980.\n",
            "  Batch   720  of    980.\n",
            "  Batch   760  of    980.\n",
            "  Batch   800  of    980.\n",
            "  Batch   840  of    980.\n",
            "  Batch   880  of    980.\n",
            "  Batch   920  of    980.\n",
            "  Batch   960  of    980.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "\n",
            "Running Validation...\n",
            "  Batch    40  of    109.\n",
            "  Batch    80  of    109.\n",
            "  Validation Loss: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "FBCmZ8iPbFt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Predicting labels for test sentences...')\n",
        "\n",
        "# Put model in evaluation mode\n",
        "eSPD.model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels, b_numer_feats = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = eSPD.model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     numerical_feats = b_numer_feats)\n",
        "\n",
        "\n",
        "  logits = result['logits']\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "metadata": {
        "id": "4espRgGUizk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f607f3ff-ffc6-468b-8d57-4c887e3919d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for test sentences...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import (\n",
        "    auc,\n",
        "    precision_recall_curve,\n",
        "    roc_auc_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    matthews_corrcoef,\n",
        ")\n",
        "\n",
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "pred_scores = softmax(flat_predictions, axis=1)[:, 1]\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "roc_auc_pred_score = roc_auc_score(flat_true_labels, pred_scores)\n",
        "precisions, recalls, thresholds = precision_recall_curve(flat_true_labels, pred_scores)\n",
        "fscore = (2 * precisions * recalls) / (precisions + recalls)\n",
        "fscore[np.isnan(fscore)] = 0\n",
        "ix = np.argmax(fscore)\n",
        "threshold = thresholds[ix].item()\n",
        "pr_auc = auc(recalls, precisions)\n",
        "tn, fp, fn, tp = confusion_matrix(flat_true_labels, flat_predictions, labels=[0, 1]).ravel()\n",
        "result = {'roc_auc': roc_auc_pred_score,\n",
        "          'threshold': threshold,\n",
        "          'pr_auc': pr_auc,\n",
        "          'recall': recalls[ix].item(),\n",
        "          'precision': precisions[ix].item(), 'f1': fscore[ix].item(),\n",
        "          'tn': tn.item(), 'fp': fp.item(), 'fn': fn.item(), 'tp': tp.item()\n",
        "          }"
      ],
      "metadata": {
        "id": "4qQOsb8jjz4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSu5lr-0sYMD",
        "outputId": "56e2dcc2-9e6d-4a21-f6f7-faddfe810e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'roc_auc': 0.9269770091406156, 'threshold': 0.3198927938938141, 'pr_auc': 0.7003751321202794, 'recall': 0.7230014025245441, 'precision': 0.6086186540731995, 'f1': 0.6608974358974359, 'tn': 11733, 'fp': 0, 'fn': 1426, 'tp': 0}\n"
          ]
        }
      ]
    }
  ]
}